{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (1.26.0)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (2.1.1+cu118)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from torchmetrics) (0.11.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.8.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torchmetrics\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset ,DataLoader, WeightedRandomSampler, random_split\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0hJiB1iwrup"
   },
   "source": [
    "# Dataset Class Definition for Non-Series Dataset\n",
    "\n",
    "In this section, we define a custom dataset class `AlzheimerDataset` for handling the Alzheimer's MRI images dataset. Each image in this dataset is treated as an individual sample. This class is crucial for loading the images and their corresponding labels efficiently during the training and evaluation of our deep learning models. It allows us to apply transformations to the dataset images and prepare them for input into the neural network.\n",
    "\n",
    "### Data Augmentation and DataLoader Preparation\n",
    "\n",
    "In this section, we initialize the dataset and then apply data augmentation techniques to increase the diversity of our dataset, which can lead to better model generalization. After initializing the Alzheimer's dataset class, we split the dataset into training, validation, and test sets. To address the issue of class imbalance, we calculate class weights and utilize these weights to create a weighted sampler for the training data. Finally, we prepare DataLoader objects for each subset of the dataset to facilitate efficient data loading during the model training and evaluation phases.\n",
    "\n",
    "To effectively handle class imbalance, we employ a weight rebalancing strategy that ensures less frequent classes are given more importance during training. This is accomplished through the following steps:\n",
    "\n",
    "1. **Calculate Class Sample Counts**: We compute the number of samples for each class in the dataset. This count helps in identifying the extent of imbalance and determining the appropriate weight that each class should receive.\n",
    "\n",
    "2. **Compute Class Weights**: For each class, the weight is calculated as the inverse of its sample count. This method assigns higher weights to less frequent classes, thus amplifying their presence during the training process.\n",
    "\n",
    "3. **Assign Sample Weights**: Each sample in the dataset is then assigned a weight corresponding to its class weight. This results in a weighted distribution where the significance of each sample is adjusted according to its weight.\n",
    "\n",
    "4. **Weighted Random Sampler**: In the training DataLoader, a WeightedRandomSampler is used with these sample weights. This sampler ensures that data batches are selected in a manner that reflects the assigned weights, providing more representation to underrepresented classes and mitigating the effects of class imbalance.\n",
    "\n",
    "By applying these steps, we ensure that the training process accounts for all classes equitably, preventing the model from biasing towards the majority classes and thereby achieving a more balanced and accurate prediction across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712322109528,
     "user": {
      "displayName": "ansar ahmed",
      "userId": "05243190925482551885"
     },
     "user_tz": -480
    },
    "id": "4jL2Wlteww7U"
   },
   "outputs": [],
   "source": [
    "class AlzheimerDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the image categories.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        categories = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "        label_mapping = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "        for category in categories:\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            for img_name in os.listdir(category_path):\n",
    "                self.image_paths.append(os.path.join(category_path, img_name))\n",
    "                self.labels.append(label_mapping[category])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label_tensor\n",
    "\n",
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Initialize the full dataset without transforms\n",
    "full_dataset = AlzheimerDataset(root_dir='Data')\n",
    "\n",
    "# Stratify split the dataset\n",
    "np_labels = np.array(full_dataset.labels)\n",
    "train_idx, val_test_idx = train_test_split(np.arange(len(np_labels)), test_size=0.3, stratify=np_labels, random_state=42)\n",
    "val_idx, test_idx = train_test_split(val_test_idx, test_size=0.5, stratify=np_labels[val_test_idx], random_state=42)\n",
    "\n",
    "# Create and transform subsets\n",
    "train_dataset = Subset(AlzheimerDataset(root_dir='Data', transform=train_transforms), train_idx)\n",
    "val_dataset = Subset(AlzheimerDataset(root_dir='Data', transform=val_test_transforms), val_idx)\n",
    "test_dataset = Subset(AlzheimerDataset(root_dir='Data', transform=val_test_transforms), test_idx)\n",
    "\n",
    "# Calculate and apply class weights\n",
    "class_weights = [1.0 / np.sum(np_labels == i) for i in range(4)]\n",
    "weights = torch.DoubleTensor([class_weights[label] for label in np_labels[train_idx]])\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FCfuc7MxZxR"
   },
   "source": [
    "# Defining a Common Training Function for All Models\n",
    "\n",
    "In this section, we define a universal training function that can be applied to all models since they share common inputs and outputs. This function will handle the model training and validation processes, including features like saving model checkpoints, tracking training and validation metrics, and optionally performing parameter search to find the optimal model configuration. The use of a common training function streamlines the training process across different models, ensuring consistency and reducing code redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1712322374066,
     "user": {
      "displayName": "ansar ahmed",
      "userId": "05243190925482551885"
     },
     "user_tz": -480
    },
    "id": "6nTOuckSxXOL"
   },
   "outputs": [],
   "source": [
    "def save_performance_metrics(performance_data, model_dir, filename):\n",
    "    \"\"\"\n",
    "    Saves performance metrics to a JSON file.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(performance_data, file)\n",
    "    print(f\"Metrics saved to {filepath}\")\n",
    "\n",
    "def save_model_checkpoint(model, model_dir, filename):\n",
    "    \"\"\"\n",
    "    Saves the model checkpoint to a file.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def handle_keyboard_interrupt(model, model_dir, performance_data, last_epoch, epoch,adv=False):\n",
    "    \"\"\"\n",
    "    Handles keyboard interrupt by saving the current model state and performance metrics.\n",
    "    \"\"\"\n",
    "    model_filename = 'interrupted.pth' if not adv else 'adv_interrupted.pth'\n",
    "    performance_filename = f'interrupted_performance_{last_epoch+1}_{epoch+1}.json'\n",
    "    save_model_checkpoint(model, model_dir, model_filename)\n",
    "    save_performance_metrics(performance_data, model_dir, performance_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, model_checkpoint_path, model_name, epochs=5, load_pretrained=False, last_epoch=-1):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_f1score = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    val_loss_history = []\n",
    "    val_accuracy_history = []\n",
    "    val_f1score_history = []\n",
    "\n",
    "    # Setup metrics\n",
    "    val_f1_score = torchmetrics.F1Score(num_classes=4, average='macro', task='multiclass').to(device)\n",
    "    val_accuracy = torchmetrics.Accuracy(num_classes=4, task='multiclass').to(device)\n",
    "\n",
    "\n",
    "    model_save_dir = os.path.join(model_checkpoint_path, model_name)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    if load_pretrained and last_epoch >= 0:\n",
    "        checkpoint_path = os.path.join(model_save_dir, f'epoch_{last_epoch}.pth')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "            print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(last_epoch + 1, last_epoch + 1 + epochs):\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "\n",
    "            for data, targets in train_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_loss_history.append(avg_train_loss)\n",
    "\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            val_f1_score.reset()\n",
    "            val_accuracy.reset()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, targets in val_loader:\n",
    "                    data, targets = data.to(device), targets.to(device)\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    predictions = outputs.argmax(dim=1)\n",
    "                    val_f1_score.update(predictions, targets)\n",
    "                    val_accuracy.update(predictions, targets)\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            current_val_f1score = val_f1_score.compute().item()\n",
    "            current_val_accuracy = val_accuracy.compute().item()\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "            val_accuracy_history.append(current_val_accuracy)\n",
    "            val_f1score_history.append(current_val_f1score)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1 Score: {current_val_f1score:.4f}, Val Accuracy: {current_val_accuracy:.4f}\")\n",
    "\n",
    "            # Save model if improved\n",
    "            if current_val_f1score > best_val_f1score or avg_val_loss < best_val_loss:\n",
    "                save_model_checkpoint(model, model_save_dir, f'epoch_{epoch}.pth')\n",
    "                best_val_f1score = current_val_f1score\n",
    "                best_val_loss = avg_val_loss\n",
    "\n",
    "        # Save metrics at the end of training\n",
    "        performance_data = {\n",
    "            'train_loss': train_loss_history,\n",
    "            'val_loss': val_loss_history,\n",
    "            'val_f1score': val_f1score_history,\n",
    "            'val_accuracy': val_accuracy_history\n",
    "        }\n",
    "        save_performance_metrics(performance_data, model_save_dir, 'final_performance_metrics.json')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "        performance_data = {\n",
    "            'train_loss': train_loss_history,\n",
    "            'val_loss': val_loss_history,\n",
    "            'val_f1score': val_f1score_history,\n",
    "            'val_accuracy': val_accuracy_history\n",
    "        }\n",
    "        handle_keyboard_interrupt(model, model_save_dir, performance_data, last_epoch, epoch)\n",
    "        print(\"All state saved after interruption.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training the Regular CNN Model\n",
    "\n",
    "In this section, we define a convolutional neural network (CNN) model with batch normalization and dropout layers to prevent overfitting and ensure more robust learning. The model comprises three convolutional layers, each followed by batch normalization, a max-pooling layer, and dropout. After the convolutional layers, the data is flattened and passed through three fully connected layers with dropout between them. This architecture is designed to learn the hierarchical features from the Alzheimer's MRI images for the classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1712322315553,
     "user": {
      "displayName": "ansar ahmed",
      "userId": "05243190925482551885"
     },
     "user_tz": -480
    },
    "id": "xvpVQiL-8jai"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained ResNet model\n",
    "resnet = models.resnet50()\n",
    "\n",
    "# Modify the first convolutional layer to accept one channel\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the final fully connected layer to output 4 classes\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 4)  # Assuming 4 classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the BatchNormDropOutCNN Model\n",
    "\n",
    "In this part of the notebook, we initialize and train the `BatchNormDropOutCNNModel` using our defined training function. We set up the model checkpoint path and model name, instantiate the model, and move it to the appropriate device (GPU if available, otherwise CPU). The model is then trained for a specified number of epochs (in this case, 200), and the performance indicators such as training loss, validation loss, training accuracy, and validation accuracy are tracked and printed. This process provides a comprehensive view of the model's performance over the training period, helping us to evaluate its effectiveness in classifying Alzheimer's disease stages from MRI images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6035, Val Loss: 1.3529, Val F1 Score: 0.3706, Val Accuracy: 0.4828\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_0.pth\n",
      "Epoch 2, Train Loss: 0.3466, Val Loss: 0.5261, Val F1 Score: 0.6754, Val Accuracy: 0.7681\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_1.pth\n",
      "Epoch 3, Train Loss: 0.2561, Val Loss: 0.6631, Val F1 Score: 0.7709, Val Accuracy: 0.7658\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_2.pth\n",
      "Epoch 4, Train Loss: 0.1998, Val Loss: 0.4204, Val F1 Score: 0.8347, Val Accuracy: 0.8317\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_3.pth\n",
      "Epoch 5, Train Loss: 0.1549, Val Loss: 0.3774, Val F1 Score: 0.8108, Val Accuracy: 0.8524\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_4.pth\n",
      "Epoch 6, Train Loss: 0.1233, Val Loss: 0.6318, Val F1 Score: 0.6995, Val Accuracy: 0.8230\n",
      "Epoch 7, Train Loss: 0.1013, Val Loss: 0.3241, Val F1 Score: 0.8892, Val Accuracy: 0.8788\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_6.pth\n",
      "Epoch 8, Train Loss: 0.0833, Val Loss: 0.3360, Val F1 Score: 0.8148, Val Accuracy: 0.8880\n",
      "Epoch 9, Train Loss: 0.0726, Val Loss: 0.0842, Val F1 Score: 0.9404, Val Accuracy: 0.9697\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_8.pth\n",
      "Epoch 10, Train Loss: 0.0626, Val Loss: 0.0714, Val F1 Score: 0.9628, Val Accuracy: 0.9753\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_9.pth\n",
      "Epoch 11, Train Loss: 0.0540, Val Loss: 0.0984, Val F1 Score: 0.9585, Val Accuracy: 0.9628\n",
      "Epoch 12, Train Loss: 0.0470, Val Loss: 0.0677, Val F1 Score: 0.9574, Val Accuracy: 0.9750\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_11.pth\n",
      "Epoch 13, Train Loss: 0.0428, Val Loss: 0.1099, Val F1 Score: 0.9113, Val Accuracy: 0.9634\n",
      "Epoch 14, Train Loss: 0.0398, Val Loss: 0.0463, Val F1 Score: 0.9769, Val Accuracy: 0.9832\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_13.pth\n",
      "Epoch 15, Train Loss: 0.0350, Val Loss: 0.0461, Val F1 Score: 0.9823, Val Accuracy: 0.9833\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_14.pth\n",
      "Epoch 16, Train Loss: 0.0309, Val Loss: 0.0542, Val F1 Score: 0.9660, Val Accuracy: 0.9802\n",
      "Epoch 17, Train Loss: 0.0296, Val Loss: 1.6103, Val F1 Score: 0.7414, Val Accuracy: 0.7330\n",
      "Epoch 18, Train Loss: 0.0283, Val Loss: 0.0292, Val F1 Score: 0.9840, Val Accuracy: 0.9890\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_17.pth\n",
      "Epoch 19, Train Loss: 0.0270, Val Loss: 0.0145, Val F1 Score: 0.9919, Val Accuracy: 0.9947\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_18.pth\n",
      "Epoch 20, Train Loss: 0.0224, Val Loss: 0.4803, Val F1 Score: 0.8766, Val Accuracy: 0.8879\n",
      "Epoch 21, Train Loss: 0.0218, Val Loss: 0.0374, Val F1 Score: 0.9801, Val Accuracy: 0.9860\n",
      "Epoch 22, Train Loss: 0.0215, Val Loss: 0.1393, Val F1 Score: 0.9239, Val Accuracy: 0.9599\n",
      "Epoch 23, Train Loss: 0.0207, Val Loss: 0.0243, Val F1 Score: 0.9861, Val Accuracy: 0.9913\n",
      "Epoch 24, Train Loss: 0.0189, Val Loss: 0.0114, Val F1 Score: 0.9950, Val Accuracy: 0.9958\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_23.pth\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_path = 'DebuggedModelCheckpoints'\n",
    "model_name = 'ResNet'\n",
    "\n",
    "# Train the model\n",
    "performance_indicators = train_model(\n",
    "    resnet, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    device, \n",
    "    model_checkpoint_path, \n",
    "    model_name, \n",
    "    epochs=200, \n",
    "    load_pretrained=False, \n",
    "    last_epoch=-1)\n",
    "print(performance_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from DebuggedModelCheckpoints/ResNet/epoch_23.pth\n",
      "Epoch 25, Train Loss: 0.0179, Val Loss: 0.0079, Val F1 Score: 0.9966, Val Accuracy: 0.9971\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_24.pth\n",
      "Epoch 26, Train Loss: 0.0186, Val Loss: 0.0120, Val F1 Score: 0.9954, Val Accuracy: 0.9958\n",
      "Epoch 27, Train Loss: 0.0158, Val Loss: 0.0075, Val F1 Score: 0.9973, Val Accuracy: 0.9977\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/epoch_26.pth\n",
      "Epoch 28, Train Loss: 0.0176, Val Loss: 0.0165, Val F1 Score: 0.9900, Val Accuracy: 0.9940\n",
      "Epoch 29, Train Loss: 0.0152, Val Loss: 0.0194, Val F1 Score: 0.9855, Val Accuracy: 0.9931\n",
      "Epoch 30, Train Loss: 0.0131, Val Loss: 0.0252, Val F1 Score: 0.9915, Val Accuracy: 0.9916\n",
      "Epoch 31, Train Loss: 0.0148, Val Loss: 0.0095, Val F1 Score: 0.9957, Val Accuracy: 0.9966\n",
      "Epoch 32, Train Loss: 0.0142, Val Loss: 0.0157, Val F1 Score: 0.9921, Val Accuracy: 0.9944\n",
      "Epoch 33, Train Loss: 0.0120, Val Loss: 0.0271, Val F1 Score: 0.9877, Val Accuracy: 0.9902\n",
      "Epoch 34, Train Loss: 0.0121, Val Loss: 0.0076, Val F1 Score: 0.9963, Val Accuracy: 0.9969\n",
      "Epoch 35, Train Loss: 0.0116, Val Loss: 0.0166, Val F1 Score: 0.9920, Val Accuracy: 0.9938\n",
      "Training interrupted by user.\n",
      "Model saved to DebuggedModelCheckpoints/ResNet/interrupted.pth\n",
      "Metrics saved to DebuggedModelCheckpoints/ResNet/interrupted_performance_24_36.json\n",
      "All state saved after interruption.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_path = 'DebuggedModelCheckpoints'\n",
    "model_name = 'ResNet'\n",
    "\n",
    "# Train the model\n",
    "performance_indicators = train_model(\n",
    "    resnet, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    device, \n",
    "    model_checkpoint_path, \n",
    "    model_name, \n",
    "    epochs=73, \n",
    "    load_pretrained=True, \n",
    "    last_epoch=23)\n",
    "print(performance_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Evaluation on Test Data\n",
    "\n",
    "After determining the best-performing epochs from both standard and adversarial training, the next step is to evaluate these selected models on the test dataset. This evaluation will give us an understanding of the models' performance in a real-world scenario, where they are exposed to unseen data.\n",
    "\n",
    "We load the model states from the best epochs identified during the validation phase and then assess their performance on the test set. This process helps in quantifying the effectiveness of the training and provides a more comprehensive view of how well the models generalize beyond the training and validation datasets.\n",
    "\n",
    "The results from this evaluation are crucial for comparing the standard and adversarially trained models, enabling us to make informed decisions regarding the deployment of these models in practical applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Import numpy for calculations\n",
    "\n",
    "def validate(model, val_loader, num_classes, adv=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.view(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    # Compute the metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Model - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Macro: {f1_macro:.4f}')\n",
    "\n",
    "    # Calculate and normalize the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize each row to sum to 1\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    # print(cm_percentage)  # Display the percentage matrix\n",
    "\n",
    "    # Plot the confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt=\".2%\", cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix for RESNET Model')\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# Load the best performing model from standard training and evaluate on test data\n",
    "path = f'DebuggedModelCheckpoints/ResNet/epoch_{26}.pth'\n",
    "\n",
    "resnet = models.resnet50()\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "\n",
    "resnet.to(device)\n",
    "\n",
    "print(\"Training Set Evaluation:\")\n",
    "validate(resnet,train_loader,4)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "validate(resnet,val_loader,4)\n",
    "print(\"Test Set Evaluation:\")\n",
    "validate(resnet,test_loader,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
